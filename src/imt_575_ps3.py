# -*- coding: utf-8 -*-
"""IMT 575 PS3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oEU93fl_zD56mNKKzlMCNYDmyygFBpp7

# **Which US Democratic Presidential Nominee said this? Warren? Biden? Sanders?**
---
### *Text Classification of quotes from candidates vying to be the Democratic presidential nominee for the 2020 US presidential election.*
---

Here, all data has been extracted from debates between candidates. We will build a NLP classification model to identify who said what for a subset of unlabeled data.


The quotes are subjected to basic text-preprocessing steps such as
1. Stopword removal
2. Punctuation removal
3. Lemmatization
4. Tokenization using unigram

To prepare data for modeling, I performed feature engineering. Here, I engineered features which utilize count of various components of the text such as character, word, punctuation etc. 

The text classification is done using **Supervised & Semi-Supervised techniques.**

The following models were explored:
1. Regularized Logistic Regression
2. Random Forest
3. XGBoost
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import re
from scipy.sparse import hstack
import string

from os import listdir
from os.path import isfile, join


import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

import xgboost as xgb
import sklearn
from sklearn import model_selection, preprocessing
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn import decomposition, ensemble
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report 

from sklearn.ensemble import RandomForestClassifier

import nltk
from nltk.tokenize import WhitespaceTokenizer 
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download("stopwords")
nltk.download("wordnet")

"""---



# Set up


---

#### 1. Load training and test dataset into dataframe

---
"""

base_path = 'drive/My Drive/Colab Notebooks/Data/'
def read_files(path):
  onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]
  name_list, file_list = [], []
  print("Reading in ", len(onlyfiles), " files.")
  for f in onlyfiles:
    file = open(path + "/" + f, 'r')
    lines  = file.readlines()
    file_list.append(lines)
    name_list.append(f.split("_")[:2])
  print("Read ", len(name_list), " files.")
  return(name_list,file_list)

#reading train data
train_path = base_path + "train"
names, file_content = read_files(train_path)

#convert to dataframe
#convert to data frame
train_all = pd.DataFrame(file_content, columns = ['c'])
split_data = train_all["c"].str.split(":")
#extract all names
c_name = []
c_quote = []
for s in split_data:
  c_name.append(s[0].split()[1].upper().replace("'", ""))
  c_quote.append(str(s[1:]))

train_df = pd.DataFrame({"label" : c_name, "Quotes": c_quote})

train_df.head()

#reading test data
test_path = base_path + "test"
test_file, test_list = read_files(test_path)

new_file = []
for f_ in test_file:
  new_file.append("_".join(f_))
#test_file

test_df = pd.DataFrame({"file_name" : new_file,"Quotes": test_list})
test_df.head()

test_df['Quotes'] = test_df['Quotes'].apply(lambda x: ','.join(map(str, x)))
test_df.head()

"""There are 528 training observations and 111 test observations

#### 2. Create a vector of training labels.

---
"""

#instances where the name of the file does not align with the name at the start of the text
count = 0
candidate_name = train_df.label
for i in range(len(names)):
  c = candidate_name[i].lower().strip()
  #print(names[i], c)
  if (names[i][0].lower().replace("'", "") != c):
    print(names[i][0], c)
    count += 1
print("There are", count, "mismatch in training files")

"""There are no observations where the candidate name in the file name and in the file content don't match."""

#find training labels
ax = sns.countplot(x='label',data=train_df)

ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha="right")
plt.tight_layout()
plt.show()

"""We can see that distribution is around ~60 for 5 candidates and for the other 5 it is around ~30 counts. The lowest count is for Tulsi Gabbard and highest for Elizabeth Warren."""

class_names = train_df.label.unique()
print("There are", len(class_names), "candidates.\nThey are as follows:", class_names)
#creating a vector of labels
# label encode the target variable 
train_y = train_df.label
encoder = preprocessing.LabelEncoder()
train_y_vector = encoder.fit_transform(train_y)
print(train_y_vector)

"""#### 3. Remove stopwords & punctuations

---
"""

#to find custom stopwords
count_words = train_df.Quotes.str.split(expand=True).stack().value_counts(normalize=True).mul(100).round(3)
for ix, c in count_words[:50].items():
  print(ix, c)

"""Here after finding the frequency of the words in the quotes, we notice that amongst the general stopwords of english languages other words such as - United, States, America, American, People appear too. It is obvious that candidates vying for the ticker will be using these words in their speeches and hence can be added to the stopwords list"""

my_stopwords = nltk.corpus.stopwords.words('english')
my_stopwords.extend(["america", "american", "united", "people"])
my_punctuation = '!"$%&\'()*+,-.…/:;<=>?[\\]^_`{|}~•@’'
def cleanQuotes(quote):
  #print(quote)
  quote = quote.lower() # lower case
  #print(quote)
  quote = quote.strip()#remove double spacing
  #quote_new = quote.translate(str.maketrans(dict.fromkeys(string.punctuation)))  
  quote = "".join([char.lower() for char in quote if char not in my_punctuation]) 
  quote = re.sub('['+my_punctuation + ']+', ' ', quote) # strip punctuation
  quote = " ".join([word for word in quote.split(' ') if word not in my_stopwords])
  return quote.strip()

train_df['clean_quote'] = train_df.Quotes.apply(cleanQuotes)

train_df.head()

"""#### 4. Stem/lemmatize your training data using a stemmer/lemmatizer of your choosing

---

Stem/lemmatize your training data using a stemmer/lemmatizer of your choosing. Show a before and after using a few observations and comment on what you see.
"""

lemmatizer = WordNetLemmatizer() 
tk = WhitespaceTokenizer() 
def lemmatizeQuotes(quote):
  quote = " ".join([lemmatizer.lemmatize(word) for word in tk.tokenize(quote)])
  return(quote)

train_df["final_clean"] = train_df.clean_quote.apply(lambda x: lemmatizeQuotes(x))

train_df.head()

for i in range(3):
  print("The text\n",train_df.iloc[i, 2])
  print("After lemmatization\n",train_df.iloc[i, 3])

"""As is observed from the texts, plurals such as mothers, victims are converted to mother, victim. I chose lemmatization as it generates valid lemma.

#### 5. Tokenize your training data using unigrams

---

Tokenize your training data using unigrams (hint: see sklearn’s CountVectorizer). If you set upper and lower limits on word frequency, what are they? How many unique tokens are in your vocabulary?
"""

vectorizer = CountVectorizer(max_df=0.95, min_df=13, ngram_range=(1, 1))
# apply transformation
train_x = vectorizer.fit_transform(train_df['final_clean'])
tf_feature_names = vectorizer.get_feature_names()
print("There are ", len(tf_feature_names), "unique tokens in our vocabulary")

"""Here, I have used a max_df of 0.95 to ignore words which occur in more than 95% of documents and min_df 13 to eliminate those which occur in less than 13 documents as there were no candidates with less than 13 quotes.

#### 6. Process the test data in a manner identical to the training data.

---
"""

test_x = vectorizer.transform(test_df.Quotes)

print("The number of feature in training is", train_x.shape[1], "and in test is" ,test_x.shape[1])

"""Therefore, the number of features for your training and test data are identical.

---
# Supervised Learning
---

#### 7. Feature Engineering


---

Here, I engineered features which utilize count of various components of the text such as character, word, punctuation etc.
"""

train_df['char_count'] = train_df['Quotes'].apply(len)
train_df['word_count'] = train_df['Quotes'].apply(lambda x: len(x.split()))
train_df['word_density'] = train_df['char_count']/(train_df['word_count'] + 1)
train_df['punctuation_count'] = train_df['Quotes'].apply(lambda x: len("".join(_ for _ in x if _ in string.punctuation))) 
train_df['title_word_count'] = train_df['Quotes'].apply(lambda x: len([word for word in x.split() if word.istitle()]))
train_df['upper_case_word_count'] = train_df['Quotes'].apply(lambda x: len([word for word in x.split() if word.isupper()]))

test_df['char_count'] = test_df['Quotes'].apply(len)
test_df['word_count'] = test_df['Quotes'].apply(lambda x: len(x.split()))
test_df['word_density'] = test_df['char_count']/(test_df['word_count']+1)
test_df['punctuation_count'] = test_df['Quotes'].apply(lambda x: len("".join(_ for _ in x if _ in string.punctuation))) 
test_df['title_word_count'] = test_df['Quotes'].apply(lambda x: len([word for word in x.split() if word.istitle()]))
test_df['upper_case_word_count'] = test_df['Quotes'].apply(lambda x: len([word for word in x.split() if word.isupper()]))

num_features = [f_ for f_ in train_df.columns\
                if f_ in ["char_count", "word_count", "word_density", 'punctuation_count','title_word_count', 'upper_case_word_count']]

for f in num_features:
    all_cut = pd.cut(pd.concat([train_df[f], test_df[f]], axis=0), bins=20, labels=False, retbins=False)
    train_df[f] = all_cut.values[:train_df.shape[0]]
    test_df[f] = all_cut.values[train_df.shape[0]:]

train_num_features = train_df[num_features].values
test_num_features = test_df[num_features].values

train_features = hstack([train_x, train_num_features]) 
test_features = hstack([test_x, test_num_features])

print("The number of features in train set", train_features.shape[1])
print("The number of features in test set", test_features.shape[1])

"""Thus, the number of features remain identical after adding 6 more features. These features were added to the countVectorizer generated features.

#### 8. Regularized Logistic Regression Model

---

Here I have used L1 regulaization for the losgistic regression model.
"""

clf_rl = LogisticRegression(penalty="l1", solver='liblinear')
clf_rl.fit(train_features,train_y)
print("For the regularized logistic regression, the coefficents are as follows:\n")
print(clf_rl.coef_)

print("The training accuracy score is", clf_rl.score(train_features, train_y)*100, "%")

"""The model performs exceptionally well in terms of accuracy. Now, lets explore class-specific metrics."""

rl_report = classification_report(train_y, clf_rl.predict(train_features))

matrix = confusion_matrix(train_y, clf_rl.predict(train_features), labels = class_names)
rl_acc = matrix.diagonal()/matrix.sum(axis=1 )
c1_df = pd.DataFrame({'accuracy': rl_acc}, index=class_names)
cm1_df = pd.DataFrame(matrix, index = class_names, columns = class_names)

print(rl_report)
print("=========================================================\n\n\nThe accuracy for each class is\n\n\n", c1_df)
print("=========================================================\n\n\nConfusion Matrix\n\n")
cm1_df.head(12)

"""As is made obvious from the confusion metrics, there are very few misclassified data points. From observing the accuracy and confusion matrix, we know that the candidate - YANG has the most misclassified quotes with a count of 5 and accuracy of 0.843. Thus, here the performance of the model is not as good as other classes where there are no to 2 misclassified points. It is also worth noting that the classifier has 100% accuracy for classes GABBARD, SANDER, BUTTIGIEG, OROURKE, STEYER alongwith almost perfect precision, recall and f1- score in some of these classes.

#### 9. Tree-based Model

---

Here, I'm using a random forest model as there ensemble component will help idenitfy good split criteria. It also decorrelates tree though at the expense of interpretability.
"""

clf_rf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=100, max_leaf_nodes=None, min_impurity_split=None,
            min_samples_leaf=3, min_samples_split=10, n_estimators=10, random_state=None)

clf_rf.fit(train_features,train_y)

print("The training accuracy score is", clf_rf.score(train_features, train_y)*100, "%")

rf_report = classification_report(train_y, clf_rf.predict(train_features))

matrix = confusion_matrix(train_y, clf_rf.predict(train_features), labels = class_names)
rf_acc = matrix.diagonal()/matrix.sum(axis=1 )
c2_df = pd.DataFrame({'accuracy': rf_acc}, index=class_names)
cm2_df = pd.DataFrame(matrix, index = class_names, columns = class_names)

print(rf_report)
print("=========================================================\n\n\nThe accuracy for each class is\n\n\n", c2_df)
print("=========================================================\n\n\nConfusion Matrix\n\n")
cm2_df.head(12)

"""As is made obvious from the confusion matrix, there are more misclassified data points than the previous model. From observing the accuracy and confusion matrix, we know that the candidate - YANG has the most misclassified quotes with accuracy of 0.375. Thus, here the performance of the model for this class is worse than the previous model. Even for other classes the accuracy is lower with highest accuracy for BIDEN, WARREN, SANDERS with good f1 scores too.

#### 10. Pick your own model

---

The model of my choice is Gradient Boosted Decision Tree which I have implemented using python's XGBoost library. I have read a lot about it and have seen it becoming a winner algo in several Kaggle competition. To tune the parameters I used a grid search to tune the depth, estimators.
"""

m_xgb = xgb.XGBClassifier(objective='binary:logistic',booster = "gbtree", eval_metric='auc')
clf_xgb = model_selection.GridSearchCV(m_xgb,{'max_depth': [4,6, 8],'n_estimators': [50,100,200]})

clf_xgb.fit(train_features,train_y)
print("The best score", clf_xgb.best_score_)

print("The training accuracy score is", clf_xgb.score(train_features, train_y)*100, "%")

xg_report = classification_report(train_y, clf_xgb.predict(train_features))

matrix = confusion_matrix(train_y, clf_xgb.predict(train_features), labels = class_names)
xg_acc = matrix.diagonal()/matrix.sum(axis=1 )
c3_df = pd.DataFrame({'accuracy': xg_acc}, index=class_names)
cm3_df = pd.DataFrame(matrix, index = class_names, columns = class_names)

print(xg_report)
print("=========================================================\n\n\nThe accuracy for each class is\n\n\n", c3_df)
print("=========================================================\n\n\nConfusion Matrix\n\n")
cm3_df.head(12)

"""As is made obvious from the confusion matrix, there are no misclassified data points. This might be due to overfitting.

#### 11. Compare the performance of the three classifiers you built

---

In my opinion, the best classifier in terms of accuracy, precision, and time is regularized logistic regression. While XGBoost has a perfect score on all evaluation metric, I'm worried it might be overfitting to the training data. In addition to this, it takes 2.5 - 3 mins to train. Thus, if I were to rank the performance only on the quantitative values while ignoring other potential issues, the ranking is - XGBoost, Reguliarized Logistic Regression, Random Forest.

However, as mentioned before I'm a bit hesistant about the perfect score of XGBoost as this might be potentially due to overfitting resulting in poor performance on the test data. Additionally, it takes 2.5- 3 min to train as compared to a few seconds taken by the other two models. Thus, the best classifier ranked then is Regularized Logistic Regression, XGBoost, Random Forest. Random Forest gives poorer metric score and it is acceptable to spend more time on XGBoost to get better result.
"""

#create graph comparing accuracy and f1 score accross different class.
x = class_names
plt.plot(c3_df.index, c3_df.accuracy)
plt.plot(c1_df.index, c1_df.accuracy)
plt.plot(c2_df.index, c2_df.accuracy)

plt.title("Class-wise Accuracy")
plt.legend(['XGBoost', 'Logistic', 'Random Forest'], loc='lower left')
plt.xticks(rotation=90)
plt.show()

"""It is obvious from the graph that while logistic regression approaches the apparent state of the art / overfitted solution of XGBoost for some classes, it also performs significantly well in those classes where random forest has almost 50% accuracy. This shows that it is doing a good job at learning features and fitting to the training data while ignoring noises and thus, should retain this while dealing with unseen test dataset.

#### 12. Generate labels

---

Here, I'm using the regularized logistic regression model to generate labels
"""

#generate labels
part_2_labels = clf_rl.predict(test_features)
print(part_2_labels)

"""---
# Semi-Supervised Learning

---

#### 13. Pick Model

---

The model of my choice is the regularized logistic regression model as it performed exceptionally well in terms of accuracy, precision, recall, f1-score and time.
"""

clf_semi = sklearn.base.clone(clf_rl, safe=True)

"""#### 14. First iteration

---
"""

#fit to un edited train data 
clf_semi.fit(train_features,train_y)

print("The accuracy of the model before iteration on the original train data is" ,clf_semi.score(train_features,train_y) * 100, "%" )

#create duplicates of train and test data frame to edit 
dup_train = train_df
#dup_train.tail(20)
dup_test = test_df    
dup_test.head()

"""Creating function to get top 11 instances of the predicted labels and to move row from test to train data"""

#get top instances 
def getTopInstances(test, testFeature, total):
  prob_predicted = {}
  k = 0
  for i, row in test.iterrows():
    prob_dict = dict(zip(clf_semi.classes_, clf_semi.predict_proba(testFeature)[k]))
    results = list(map(lambda x: (x[0], x[1]), sorted(zip(clf_semi.classes_, clf_semi.predict_proba(testFeature)[k]), key=lambda x: x[1], reverse=True)))
    prob_predicted[i] = results[0]
    k += 1
  print("The class and their associated probability", prob_predicted)
  sort_prob = sorted(prob_predicted.keys(), key=lambda x: prob_predicted[x][1], reverse=True)[:total]
  return(sort_prob, prob_predicted)

#get top 10 index and classes
def addTopTest(test, train, prob_class, prob_pred):
  #find label class and index to add
  k = 1
  l = 11
  for i in prob_class:
    curr_row = []
    #remove from the test features
    curr_s= test.loc[[i]]
    #drop these rows from the data frame
    test = test.drop([i])
    clean = cleanQuotes(curr_s.Quotes.item())
    final_clean = lemmatizeQuotes(clean)
    curr_row.append([prob_pred[i][0], curr_s.Quotes.item(), clean, final_clean, curr_s.char_count.item(), curr_s.word_count.item(), curr_s.word_density.item(),\
                     curr_s.punctuation_count.item(), curr_s.title_word_count.item(), curr_s.upper_case_word_count.item()])
    #add curr_row to training df
    train = train.append(pd.DataFrame(curr_row, columns = train.columns))
    print("Adding row ",i, " ----- ", k, "/", l)
    k += 1
    #print("Adding row\n",curr_row)
    #print("\nShape after adding row", train.shape)
  return(train, test)

pc, pp = getTopInstances(dup_test, test_features, 11)

#generate new train and test
dup_train, dup_test = addTopTest(dup_test, dup_train, pc, pp)

print("The shape of train:", dup_train.shape, "\ntest:", dup_test.shape)

#vectorize and then 
#vectorize train and test both 
def generateFeatures(train, test):
  semi_train_x = vectorizer.transform(train['final_clean'])
  semi_test_x = vectorizer.transform(test.Quotes)
  semi_features = [f_ for f_ in train.columns\
                  if f_ in ["char_count", "word_count", "word_density", 'punctuation_count','title_word_count', 'upper_case_word_count']]
  print("The shape of train:", semi_train_x.shape, "\ntest:", semi_train_x .shape)


  for f in semi_features:
      all_cut = pd.cut(pd.concat([train[f], test[f]], axis=0), bins=20, labels=False, retbins=False)
      train[f] = all_cut.values[:train.shape[0]]
      test[f] = all_cut.values[train.shape[0]:]

  train_dup_features = train[semi_features].values
  test_dup_features = test[semi_features].values
  dup_train_features = hstack([semi_train_x, train_dup_features]) 
  dup_test_features = hstack([semi_test_x, test_dup_features])

  return(dup_train_features, dup_test_features)

#generate feature for new test train
curr_train_feature, curr_test_feature = generateFeatures(dup_train, dup_test)

clf_semi.fit(curr_train_feature,dup_train.label)

accuracy = []
ac = clf_semi.score(train_features, train_df.label)
accuracy.append(ac)
print("The accuracy of the model after 1st iteration on the updated train data is" , clf_semi.score(curr_train_feature, dup_train.label) * 100, "%" )
print("The accuracy of the model after 1st iteration on the original train data is" ,ac * 100, "%" )

"""The accuracy of the model for the original train data is 97.15% which is an increase from 96.96% which observed in part 2 or at the start of part 3.

#### 15. 9 more iteration and plotting performance in terms of accuracy

---

15. Now, repeat the process above 9 more times, each time adding an additional 10% of the test data for which the labels have the highest probability (note that this may result in different test observations being included from one iteration to the next). After each iteration, note the performance on the original training data set. Generate a plot which shows the percentage of the test dataset used on the X-axis and the classification accuracy on the original training dataset on the Y-axis. There should be 11 points for this plot, ranging from X = 0% to X = 100%, inclusive. What does the plot look like? Comment on what you see. (Note: this type of semi-supervised learning can very much be hit-or-miss. This type of learning doesn’t always yield benefits).
"""

for ix in range(2, 11):
  print("===============================" , ix, "===================================")
  if ix == 10:
    pc, pp = getTopInstances(dup_test, curr_test_feature, 12)
  else:
    pc, pp = getTopInstances(dup_test, curr_test_feature, 11)
  #generate new train and test
  dup_train, dup_test = addTopTest(dup_test, dup_train, pc, pp)
  print("The shape of train:", dup_train.shape, "\ntest:", dup_test.shape)
  #generate feature for new test train
  curr_train_feature, curr_test_feature = generateFeatures(dup_train, dup_test)
  clf_semi.fit(curr_train_feature,dup_train.label)
  ac = clf_semi.score(train_features, train_df.label)
  accuracy.append(ac)
  print("The accuracy of the model after", ix," iteration on the update train data is" , clf_semi.score(curr_train_feature, dup_train.label) * 100, "%" )
  print("The accuracy of the model after", ix,"iteration on the original train data is" ,ac * 100, "%" )

print(len(accuracy))

plt.axes()
plt.plot(np.arange(10, 110, 10), accuracy)
plt.xlim([0, 120])
plt.ylim([0.95, 1])
plt.yticks(np.arange(0.95, 1, 0.01))
plt.xticks(np.arange(10, 110, 10))

plt.title("Classification Accuracy vs % test data")

plt.xticks(rotation=90)
plt.xlabel("Percentage of test data added to the train set")
plt.ylabel("Classification accuracy of original train set")
plt.show()
plt.show()

"""As we keep adding self-labelled test data to the training data and train on this new data, we notice that the accuracy for the original train data which initially now follows a decrease followed by increase pattern. Towards the end, the accuracy of the model for the original train data is a little less than the original regularized logistic regression model

#### 16. Generate labels for semi-supervised

---
"""

part_3_labels = clf_semi.predict(test_features)
print(part_3_labels)

#sanity check
print("The length of the predicted values ", len(part_3_labels))
#extract file name from test_df
file_label = test_df.file_name.tolist()
print(file_label)

#convert to a dataframe
final_submission = pd.DataFrame({'FILE': file_label, 'MODEL1':part_2_labels, 'MODEL2': part_3_labels})
final_submission.head()

#sorting by file name
submission_df = final_submission.sort_values('FILE')

submission_df.head()

#CONVERT TO TXT FILE
final_submission.to_csv('submission.txt', sep='\t', index=False)